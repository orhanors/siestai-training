{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a734ac7-20c3-4883-991b-2991c53270d5",
   "metadata": {},
   "source": [
    "# Agent Memory - Can LLMs *Really* Think?\n",
    "\n",
    "<img src=\"./media/memory.png\" width=600>\n",
    "\n",
    "*[Cognitive Architectures for Language Agents, 2024](https://arxiv.org/pdf/2309.02427)*\n",
    "\n",
    "LLMs are considered \"stateless\" in that every time you invoke an LLM call, it is like the first time it's ever seen the input being passed through. Given this quirk, multi-turn LLM agents have a unique challenge to overcome with fully understanding and navigating a vast world model which we humans do naturally.\n",
    "\n",
    "Being a human has a lot of advantages over a language model when executing a task. We bring our general knowledge about the world and lived experience, our understanding of prior similar task experiences and their takeaways, what we've specifically learned how to do or been taught, and then our ability to instantly contextualize and shape our approach to a task as we're working through it. In essence, we have advanced memory and the ability to learn from and apply learnings to new experiences. \n",
    "\n",
    "LLMs sort of have some memory, mostly their general knowledge or traits picked up from training and additional fine tuning but suffer from a lack of the other characteristics outlined prior. To compensate for this, we can model different forms of memory, recall, and learning within our agentic system design. Specifically, we'll create a simple RAG agent to model 4 kinds of memory:\n",
    "\n",
    "- **Working Memory** - Current conversation and immediate context\n",
    "- **Episodic Memory** - Historical experiences and their takeaways\n",
    "- **Semantic Memory** - Knowledge context and factual grounding\n",
    "- **Procedural Memory** - The \"rules\" and \"skills\" for interaction\n",
    "\n",
    "These four memory systems provide a holistic approach to understanding and architecting a part of cognitive design into an agent application. In this notebook we'll break down each type of memory and an example approach to implementing them into a whole agent experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb62c6e-e56c-4fee-bb3d-9329de4f79f4",
   "metadata": {},
   "source": [
    "---\n",
    "## Working Memory\n",
    "\n",
    "<img src=\"./media/working_memory.png\" width=400>\n",
    "\n",
    "*[Working Memory Model (Baddeley and Hitch)](https://www.simplypsychology.org/working-memory.html)*\n",
    "\n",
    "Working memory encompasses your active understanding and contextualization of immediate information requiring dynamic processing. For a chatbot, this represents the maintenance and manipulation of conversational context observed throughout real-time interactions.\n",
    "\n",
    "The type of information maintained in working memory consists of active messages and roles, current task/goal parameters, immediate state representations, and contextual processing requirements. This includes message history with associated metadata, conversation state vectors, goal hierarchies, and temporary computational results requiring immediate access.\n",
    "\n",
    "```python\n",
    "chat_model.invoke([\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful AI Assistant.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello, how are you?\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"I'm doing well, thank you for asking.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Can you tell me a joke?\",\n",
    "    }\n",
    "])\n",
    "```\n",
    "\n",
    "Remembering from working memory involves direct access to recent contextual data and action/result pairs. The system leverages immediate accessibility to maintain conversational coherence through continuous monitoring of the active message history, current state parameters, and ongoing computational processes. This direct access enables appropriate response generation grounded in the immediate conversational context.\n",
    "\n",
    "Learning in working memory operates through continuous state updates during conversational processing. The system dynamically integrates new messages into the active context, updates state representations, modifies goal parameters, and maintains temporal coherence across the interaction. This real-time learning process differs fundamentally from the persistent storage mechanisms of episodic and semantic memory systems.\n",
    "\n",
    "Working memory functions as the active computational interface, coordinating information flow between episodic experience retrieval and semantic knowledge access while maintaining precise state awareness of the current interaction.\n",
    "\n",
    "<img src=\"./media/working_diagram.png\" width=450>\n",
    "\n",
    "Looking at a simple example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7d5cc0-35d8-4f94-a68e-d0876417de7b",
   "metadata": {},
   "source": [
    "**Instantiate the Language Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3145ac1f-81bf-4aa0-a542-1ebc24c2ac1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.7, model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83ca92a-70c4-4d94-afcd-4910d8a83bb2",
   "metadata": {},
   "source": [
    "**Create Simple Back & Forth Chat Flow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3d3fa7-2109-449b-95f6-077b10622704",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Define System Prompt\n",
    "system_prompt = SystemMessage(\"You are a helpful AI Assistant. Answer the User's queries succinctly in one sentence.\")\n",
    "\n",
    "# Start Storage for Historical Message History\n",
    "messages = [system_prompt]\n",
    "\n",
    "while True:\n",
    "\n",
    "    # Get User's Message\n",
    "    user_message = HumanMessage(input(\"\\nUser: \"))\n",
    "    \n",
    "    if user_message.content.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    else:\n",
    "        # Extend Messages List With User Message\n",
    "        messages.append(user_message)\n",
    "\n",
    "    # Pass Entire Message Sequence to LLM to Generate Response\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    print(\"\\nAI Message: \", response.content)\n",
    "\n",
    "    # Add AI's Response to Message List\n",
    "    messages.append(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f0bb18-730e-4bf7-bc91-d60f60c28d32",
   "metadata": {},
   "source": [
    "Keeping track of our total conversation allows the LLM to use prior messages and interactions as context for immediate responses during an ongoing conversation, keeping our current interaction in working memory and recalling working memory through attaching it as context for subsequent response generations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dddc2a-9ad6-49e5-8e1a-1cfad3f358da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking into our Memory\n",
    "\n",
    "for i in range(len(messages)):\n",
    "    print(f\"\\nMessage {i+1} - {messages[i].type.upper()}: \", messages[i].content)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5867cf90-fafd-444e-bb0b-f9b52be7836d",
   "metadata": {},
   "source": [
    "---\n",
    "## Episodic Memory\n",
    "\n",
    "<img src=\"./media/episodic_memory.png\" width=400>\n",
    "\n",
    "*[Tell me why: the missing w in episodic memoryâ€™s what, where, and when](https://link.springer.com/article/10.3758/s13415-024-01234-4)*\n",
    "\n",
    "Episodic memory is a historical collection of prior experiences, or episodes. This can be both the literal recollection of how something happened and also any non-explicitly stated takeaways. When encountering a specific situation, you may recall similar related events that you've been in and their outcomes, which shape the way we approach new, comparable experiences.\n",
    "\n",
    "For a chatbot, this includes both raw conversations it has participated in and the analytical understanding gained from those interactions. The act of remembering is implemented through dynamic [few-shot prompting](https://www.promptingguide.ai/techniques/fewshot), automatically providing similar successful examples and instructions to better guide an LLM's response on subsequent similar queries.\n",
    "\n",
    "But we don't just recall similar experiences - we also extract takeaways (or learning) from interactions. Learning in episodic memory happens through two processes: automatic storage of complete conversations, and generation of post-conversation analysis. The system stores full interaction sequences while implementing reflection protocols to identify what worked, what didn't, and what can be learned for future situations. This dual approach enables both specific recall and strategic learning for future conversations.\n",
    "\n",
    "<img src=\"./media/episodic_diagram_1.png\" width=600>\n",
    "\n",
    "Episodic memory serves as the system's experiential foundation, allowing it to adapt its behavior based on accumulated conversation history while maintaining access to proven interaction patterns and their associated learnings. This creates a continuously improving system that learns not just from individual interactions, but from the patterns and insights derived across multiple conversations.\n",
    "\n",
    "Let's implement this reflection, storage and retrieval:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41446e2b-ace3-4221-a64e-f76d5079f07b",
   "metadata": {},
   "source": [
    "**Creating a Reflection Chain**\n",
    "\n",
    "This is where historical messages can be input, and episodic memories will be output. Given a message history, you will receive\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"context_tags\": [               # 2-4 keywords that would help identify similar future conversations\n",
    "        string,                     # Use field-specific terms like \"deep_learning\", \"methodology_question\", \"results_interpretation\"\n",
    "        ...\n",
    "    ],\n",
    "    \"conversation_summary\": string, # One sentence describing what the conversation accomplished\n",
    "    \"what_worked\": string,          # Most effective approach or strategy used in this conversation\n",
    "    \"what_to_avoid\": string         # Most important pitfall or ineffective approach to avoid\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c234c48f-54d9-4bc7-b620-88c7c38d1a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "reflection_prompt_template = \"\"\"\n",
    "You are analyzing conversations about research papers to create memories that will help guide future interactions. Your task is to extract key elements that would be most helpful when encountering similar academic discussions in the future.\n",
    "\n",
    "Review the conversation and create a memory reflection following these rules:\n",
    "\n",
    "1. For any field where you don't have enough information or the field isn't relevant, use \"N/A\"\n",
    "2. Be extremely concise - each string should be one clear, actionable sentence\n",
    "3. Focus only on information that would be useful for handling similar future conversations\n",
    "4. Context_tags should be specific enough to match similar situations but general enough to be reusable\n",
    "\n",
    "Output valid JSON in exactly this format:\n",
    "{{\n",
    "    \"context_tags\": [              // 2-4 keywords that would help identify similar future conversations\n",
    "        string,                    // Use field-specific terms like \"deep_learning\", \"methodology_question\", \"results_interpretation\"\n",
    "        ...\n",
    "    ],\n",
    "    \"conversation_summary\": string, // One sentence describing what the conversation accomplished\n",
    "    \"what_worked\": string,         // Most effective approach or strategy used in this conversation\n",
    "    \"what_to_avoid\": string        // Most important pitfall or ineffective approach to avoid\n",
    "}}\n",
    "\n",
    "Examples:\n",
    "- Good context_tags: [\"transformer_architecture\", \"attention_mechanism\", \"methodology_comparison\"]\n",
    "- Bad context_tags: [\"machine_learning\", \"paper_discussion\", \"questions\"]\n",
    "\n",
    "- Good conversation_summary: \"Explained how the attention mechanism in the BERT paper differs from traditional transformer architectures\"\n",
    "- Bad conversation_summary: \"Discussed a machine learning paper\"\n",
    "\n",
    "- Good what_worked: \"Using analogies from matrix multiplication to explain attention score calculations\"\n",
    "- Bad what_worked: \"Explained the technical concepts well\"\n",
    "\n",
    "- Good what_to_avoid: \"Diving into mathematical formulas before establishing user's familiarity with linear algebra fundamentals\"\n",
    "- Bad what_to_avoid: \"Used complicated language\"\n",
    "\n",
    "Additional examples for different research scenarios:\n",
    "\n",
    "Context tags examples:\n",
    "- [\"experimental_design\", \"control_groups\", \"methodology_critique\"]\n",
    "- [\"statistical_significance\", \"p_value_interpretation\", \"sample_size\"]\n",
    "- [\"research_limitations\", \"future_work\", \"methodology_gaps\"]\n",
    "\n",
    "Conversation summary examples:\n",
    "- \"Clarified why the paper's cross-validation approach was more robust than traditional hold-out methods\"\n",
    "- \"Helped identify potential confounding variables in the study's experimental design\"\n",
    "\n",
    "What worked examples:\n",
    "- \"Breaking down complex statistical concepts using visual analogies and real-world examples\"\n",
    "- \"Connecting the paper's methodology to similar approaches in related seminal papers\"\n",
    "\n",
    "What to avoid examples:\n",
    "- \"Assuming familiarity with domain-specific jargon without first checking understanding\"\n",
    "- \"Over-focusing on mathematical proofs when the user needed intuitive understanding\"\n",
    "\n",
    "Do not include any text outside the JSON object in your response.\n",
    "\n",
    "Here is the prior conversation:\n",
    "\n",
    "{conversation}\n",
    "\"\"\"\n",
    "\n",
    "reflection_prompt = ChatPromptTemplate.from_template(reflection_prompt_template)\n",
    "\n",
    "reflect = reflection_prompt | llm | JsonOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d823b6-2752-4bb1-9e54-9fcb29fc78c7",
   "metadata": {},
   "source": [
    "**Format Conversation Helper Function**\n",
    "\n",
    "Cleans up the conversation by removing the system prompt, effectively only returning a string of the relevant conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380b3302-cd69-4e4a-9b44-eaf3209394bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_conversation(messages):\n",
    "    \n",
    "    # Create an empty list placeholder\n",
    "    conversation = []\n",
    "    \n",
    "    # Start from index 1 to skip the first system message\n",
    "    for message in messages[1:]:\n",
    "        conversation.append(f\"{message.type.upper()}: {message.content}\")\n",
    "    \n",
    "    # Join with newlines\n",
    "    return \"\\n\".join(conversation)\n",
    "\n",
    "conversation = format_conversation(messages)\n",
    "\n",
    "print(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce260e05-fc34-4e89-8c0a-9dc288888da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection = reflect.invoke({\"conversation\": conversation})\n",
    "\n",
    "print(reflection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36e9655-ab73-4cb8-86fb-5b16d2802320",
   "metadata": {},
   "source": [
    "**Setting Up our Database**\n",
    "\n",
    "This will act as our memory store, both for \"remembering\" and for \"recalling\". \n",
    "\n",
    "We will be using [weviate](https://weaviate.io/) with [ollama embeddings](https://ollama.com/library/nomic-embed-text) running in a docker container. See [docker-compose.yml](./docker-compose.yml) for additional details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc980dd-3c93-465c-b074-f177b73b760f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "\n",
    "vdb_client = weaviate.connect_to_local()\n",
    "print(\"Connected to Weviate: \", vdb_client.is_ready())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6264ee17-46c3-4d01-9aff-3601af0817f8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# vdb_client.collections.delete(\"episodic_memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2319ecab-944c-47b9-8dc7-8fdf4b29329c",
   "metadata": {},
   "source": [
    "**Create an Episodic Memory Collection**\n",
    "\n",
    "These are the individual memories that we'll be able to search over. \n",
    "\n",
    "We note down `conversation`, `context_tags`, `conversation_summary`, `what_worked`, and `what_to_avoid` for each entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7224de-f9a5-4822-9caf-2151088b351a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.classes.config import Property, DataType, Configure, Tokenization\n",
    "\n",
    "vdb_client.collections.create(\n",
    "    name=\"episodic_memory\",\n",
    "    description=\"Collection containing historical chat interactions and takeaways.\",\n",
    "    vectorizer_config=[\n",
    "        Configure.NamedVectors.text2vec_ollama(\n",
    "            name=\"title_vector\",\n",
    "            source_properties=[\"title\"],\n",
    "            api_endpoint=\"http://host.docker.internal:11434\",  # If using Docker, use this to contact your local Ollama instance\n",
    "            model=\"nomic-embed-text\",\n",
    "        )\n",
    "    ],\n",
    "    properties=[\n",
    "        Property(name=\"conversation\", data_type=DataType.TEXT),\n",
    "        Property(name=\"context_tags\", data_type=DataType.TEXT_ARRAY),\n",
    "        Property(name=\"conversation_summary\", data_type=DataType.TEXT),\n",
    "        Property(name=\"what_worked\", data_type=DataType.TEXT),\n",
    "        Property(name=\"what_to_avoid\", data_type=DataType.TEXT),\n",
    "        \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011fb9c5-54f5-4aeb-9334-91e4f6a91f5d",
   "metadata": {},
   "source": [
    "**Helper Function for Remembering an Episodic Memory**\n",
    "\n",
    "Takes in a conversation, creates a reflection, then adds it to the database collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07fa1ec8-f252-497e-a88c-ebdf5d199298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_episodic_memory(messages, vdb_client):\n",
    "\n",
    "    # Format Messages\n",
    "    conversation = format_conversation(messages)\n",
    "\n",
    "    # Create Reflection\n",
    "    reflection = reflect.invoke({\"conversation\": conversation})\n",
    "\n",
    "    # Load Database Collection\n",
    "    episodic_memory = vdb_client.collections.get(\"episodic_memory\")\n",
    "\n",
    "    # Insert Entry Into Collection\n",
    "    episodic_memory.data.insert({\n",
    "        \"conversation\": conversation,\n",
    "        \"context_tags\": reflection['context_tags'],\n",
    "        \"conversation_summary\": reflection['conversation_summary'],\n",
    "        \"what_worked\": reflection['what_worked'],\n",
    "        \"what_to_avoid\": reflection['what_to_avoid'],\n",
    "    })\n",
    "\n",
    "# add_episodic_memory(messages, vdb_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace1b0fd-8336-4a5f-88a0-1ec4f458a2ec",
   "metadata": {},
   "source": [
    "**Episodic Memory Remembering/Recall Function**\n",
    "\n",
    "Queries our episodic memory collection and return's back the most relevant result using hybrid semantic & BM25 search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ac2001-32e0-461a-849e-344a0dc40eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def episodic_recall(query, vdb_client):\n",
    "    \n",
    "    # Load Database Collection\n",
    "    episodic_memory = vdb_client.collections.get(\"episodic_memory\")\n",
    "\n",
    "    # Hybrid Semantic/BM25 Retrieval\n",
    "    memory = episodic_memory.query.hybrid(\n",
    "        query=query,\n",
    "        alpha=0.5,\n",
    "        limit=1,\n",
    "    )\n",
    "    \n",
    "    return memory\n",
    "\n",
    "query = \"Talking about my name\"\n",
    "\n",
    "memory = episodic_recall(query, vdb_client)\n",
    "\n",
    "memory.objects[0].properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9ba877-33df-4a97-983c-aaf116a7f597",
   "metadata": {},
   "source": [
    "**Episodic Memory System Prompt Function**\n",
    "\n",
    "Takes in the memory and modifies the system prompt, dynamically inserting the latest conversation, including the last 3 conversations, keeping a running list of what worked and what to avoid.\n",
    "\n",
    "This will allow us to update the LLM's behavior based on it's 'recollection' of episodic memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "593f18b3-daaa-46d1-8ac6-ace095f8f7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def episodic_system_prompt(query, vdb_client):\n",
    "    # Get new memory\n",
    "    memory = episodic_recall(query, vdb_client)\n",
    "\n",
    "    current_conversation = memory.objects[0].properties['conversation']\n",
    "    # Update memory stores, excluding current conversation from history\n",
    "    if current_conversation not in conversations:\n",
    "        conversations.append(current_conversation)\n",
    "    # conversations.append(memory.objects[0].properties['conversation'])\n",
    "    what_worked.update(memory.objects[0].properties['what_worked'].split('. '))\n",
    "    what_to_avoid.update(memory.objects[0].properties['what_to_avoid'].split('. '))\n",
    "\n",
    "    # Get previous conversations excluding the current one\n",
    "    previous_convos = [conv for conv in conversations[-4:] if conv != current_conversation][-3:]\n",
    "    \n",
    "    # Create prompt with accumulated history\n",
    "    episodic_prompt = f\"\"\"You are a helpful AI Assistant. Answer the user's questions to the best of your ability.\n",
    "    You recall similar conversations with the user, here are the details:\n",
    "    \n",
    "    Current Conversation Match: {memory.objects[0].properties['conversation']}\n",
    "    Previous Conversations: {' | '.join(previous_convos)}\n",
    "    What has worked well: {' '.join(what_worked)}\n",
    "    What to avoid: {' '.join(what_to_avoid)}\n",
    "    \n",
    "    Use these memories as context for your response to the user.\"\"\"\n",
    "    \n",
    "    return SystemMessage(content=episodic_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0639cf56-8864-48e8-900a-840751906fe4",
   "metadata": {},
   "source": [
    "**Episodic Memory + Working Memory Demonstration**\n",
    "\n",
    "<img src=\"./media/episodic_diagram_2.png\" width=800>\n",
    "\n",
    "Current flow will:\n",
    "1. Take a user's message\n",
    "2. Create a system prompt with relevant Episodic enrichment\n",
    "3. Reconstruct the entire working memory to update the system prompt and attach the new message to the end\n",
    "4. Generate a response with the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405bf2cc-e8be-4e6d-be67-536bec8a3636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple storage for accumulated memories\n",
    "conversations = []\n",
    "what_worked = set()\n",
    "what_to_avoid = set()\n",
    "\n",
    "# Start Storage for Historical Message History\n",
    "messages = []\n",
    "\n",
    "while True:\n",
    "    # Get User's Message\n",
    "    user_input = input(\"\\nUser: \")\n",
    "    user_message = HumanMessage(content=user_input)\n",
    "    \n",
    "    # Generate new system prompt\n",
    "    system_prompt = episodic_system_prompt(user_input, vdb_client)\n",
    "    \n",
    "    # Reconstruct messages list with new system prompt first\n",
    "    messages = [\n",
    "        system_prompt,  # New system prompt always first\n",
    "        *[msg for msg in messages if not isinstance(msg, SystemMessage)]  # Old messages except system\n",
    "    ]\n",
    "    \n",
    "    if user_input.lower() == \"exit\":\n",
    "        add_episodic_memory(messages, vdb_client)\n",
    "        print(\"\\n == Conversation Stored in Episodic Memory ==\")\n",
    "        break\n",
    "    if user_input.lower() == \"exit_quiet\":\n",
    "        print(\"\\n == Conversation Exited ==\")\n",
    "        break\n",
    "    \n",
    "    # Add current user message\n",
    "    messages.append(user_message)\n",
    "    \n",
    "    # Pass Entire Message Sequence to LLM to Generate Response\n",
    "    response = llm.invoke(messages)\n",
    "    print(\"\\nAI Message: \", response.content)\n",
    "    \n",
    "    # Add AI's Response to Message List\n",
    "    messages.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c726f9b7-10e3-49b5-9817-28f0e5f2c8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking into our Memory\n",
    "\n",
    "for i in range(len(messages)):\n",
    "    print(f\"\\nMessage {i+1} - {messages[i].type.upper()}: \", messages[i].content)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c86cbbd-da2e-433e-b66d-bd05ba93034b",
   "metadata": {},
   "source": [
    "---\n",
    "## Semantic Memory\n",
    "\n",
    "<img src=\"./media/semantic_memory.png\" width=400>\n",
    "\n",
    "*[Recognition-induced forgetting is caused by episodic, not semantic, memory retrieval tasks](https://link.springer.com/article/10.3758/s13414-020-01987-3)*\n",
    "\n",
    "Semantic memory represents our structured knowledge of facts, concepts, and their relationships - essentially what we \"know\" rather than what we \"remember experiencing.\" This type of memory allows us to understand and interact with the world by accessing our accumulated knowledge. For a chatbot, semantic memory would consist of its knowledge base and retrieval system, containing documentation, technical information, and general knowledge that can be accessed to provide accurate and informed responses.\n",
    "\n",
    "The key difference from episodic memory is that semantic memory isn't tied to specific experiences or events - it's about understanding concepts and facts in an abstract way. In an AI system, this would be implemented through techniques like Retrieval Augmented Generation (RAG), where relevant information is dynamically pulled from a knowledge base to ground and inform responses.\n",
    "\n",
    "Learning in semantic memory involves expanding and refining the knowledge base - adding new information, updating existing entries, and broadening coverage of different topics. This could mean incorporating new documentation, updating technical specifications, or expanding the range of topics the system can knowledgeably discuss. The act of remembering then becomes a process of retrieving and synthesizing relevant information from this knowledge base to provide accurate and contextual responses.\n",
    "\n",
    "This semantic knowledge can then be combined with the current conversation context (working memory) and past similar interactions (episodic memory) to provide comprehensive, accurate, and contextually appropriate responses. The system not only knows what it's talking about (semantic memory) but can relate it to the current conversation (working memory) and past experiences (episodic memory)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4170025-86fa-402d-ae76-9454f62b77e0",
   "metadata": {},
   "source": [
    "**Creating our Knowledgebase**\n",
    "\n",
    "For our semantic knowledge, we'll be chunking the [Cognitive Architectures for Language Agents paper](https://arxiv.org/pdf/2309.02427). This will become the facts and concepts that we will dynamically \"remember\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b84b25-3159-4d73-a839-9f0a23221a70",
   "metadata": {},
   "source": [
    "**Custom Chunking**\n",
    "\n",
    "Taking advantage of [ChromaDB's custom chunkers](https://research.trychroma.com/evaluating-chunking), using a recursive character chunker to split the document text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81419d3-5285-4825-9263-24874cfc0728",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install git+https://github.com/brandonstarxel/chunking_evaluation.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55b3180-477f-499e-bde4-b4e76f584134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chunking_evaluation.chunking import RecursiveTokenChunker\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"./CoALA_Paper.pdf\")\n",
    "pages = []\n",
    "for page in loader.load():\n",
    "    pages.append(page)\n",
    "\n",
    "# Combine all page contents into one string\n",
    "document = \" \".join(page.page_content for page in pages)\n",
    "\n",
    "# Set up the chunker with your specified parameters\n",
    "recursive_character_chunker = RecursiveTokenChunker(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=0,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"?\", \"!\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Split the combined text\n",
    "recursive_character_chunks = recursive_character_chunker.split_text(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c98047e-d16a-4b55-8a67-a92828ad1b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(recursive_character_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4bf9fe-3e60-4752-a954-5d5142e753d4",
   "metadata": {},
   "source": [
    "**Creating our Semantic Memory Collection**\n",
    "\n",
    "Additional collection within our weviate, this time just holding the individual chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ef6d21-26e4-449c-b7a1-886924ef193b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vdb_client.collections.delete(\"CoALA_Paper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55bb421-4975-4af6-9e49-c1f73d7cc58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdb_client.collections.create(\n",
    "    name=\"CoALA_Paper\",\n",
    "    description=\"Collection containing split chunks from the CoALA Paper\",\n",
    "    vectorizer_config=[\n",
    "        Configure.NamedVectors.text2vec_ollama(\n",
    "            name=\"title_vector\",\n",
    "            source_properties=[\"title\"],\n",
    "            api_endpoint=\"http://host.docker.internal:11434\",  # If using Docker, use this to contact your local Ollama instance\n",
    "            model=\"nomic-embed-text\",\n",
    "        )\n",
    "    ],\n",
    "    properties=[\n",
    "        Property(name=\"chunk\", data_type=DataType.TEXT),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad40e615-a3bc-4258-ba3a-a26d37fb9f4c",
   "metadata": {},
   "source": [
    "**Inserting Chunked Paper into Collection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c232e679-a0a2-4124-b29f-719f54ad5b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Database Collection\n",
    "coala_collection = vdb_client.collections.get(\"CoALA_Paper\")\n",
    "\n",
    "for chunk in recursive_character_chunks:\n",
    "    # Insert Entry Into Collection\n",
    "    coala_collection.data.insert({\n",
    "        \"chunk\": chunk,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebd7303-bebf-485a-964e-1ad09ad0adf3",
   "metadata": {},
   "source": [
    "**Semantic Recall Function**\n",
    "\n",
    "This retrieval function queries our knowledgebase of the CoALA paper and combines all of the retrieved chunks into one large string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e50c76a0-1319-4eef-9e8b-6a380f58bab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_recall(query, vdb_client):\n",
    "    \n",
    "    # Load Database Collection\n",
    "    coala_collection = vdb_client.collections.get(\"CoALA_Paper\")\n",
    "\n",
    "    # Hybrid Semantic/BM25 Retrieval\n",
    "    memories = coala_collection.query.hybrid(\n",
    "        query=query,\n",
    "        alpha=0.5,\n",
    "        limit=15,\n",
    "    )\n",
    "\n",
    "    combined_text = \"\"\n",
    "    \n",
    "    for i, memory in enumerate(memories.objects):\n",
    "        # Add chunk separator except for first chunk        if i > 0:\n",
    "\n",
    "        \n",
    "        # Add chunk number and content\n",
    "        combined_text += f\"\\nCHUNK {i+1}:\\n\"\n",
    "        combined_text += memory.properties['chunk'].strip()\n",
    "    \n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c7c7f7-7fec-4ba2-9eef-8219fe91f5d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "memories = semantic_recall(\"What are the four kinds of memory\", vdb_client)\n",
    "\n",
    "print(memories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb39fe60-c614-472c-894b-ea65d33d8d9e",
   "metadata": {},
   "source": [
    "**Formatting the Semantic Memory**\n",
    "\n",
    "Attaching additional instructions along with the retrieved chunks. This will be an additional human message that we'll put in and out with every message, updating with the latest context retrieved from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a1f0f3e-fe48-4d62-ac45-a2bfee4c38fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_rag(query, vdb_client):\n",
    "\n",
    "    memories = semantic_recall(query, vdb_client)\n",
    "\n",
    "    semantic_prompt = f\"\"\" If needed, Use this grounded context to factually answer the next question.\n",
    "    Let me know if you do not have enough information or context to answer a question.\n",
    "    \n",
    "    {memories}\n",
    "    \"\"\"\n",
    "    \n",
    "    return HumanMessage(semantic_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f99d2d5-39bc-41df-a688-f0d93b4de68d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "message = semantic_rag(\"What are the four kinds of memory\", vdb_client)\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dca139-d46b-4e3f-be66-80d342d12551",
   "metadata": {},
   "source": [
    "**Semantic Memory with Episodic and Working Memory Demonstration**\n",
    "\n",
    "<img src=\"./media/semantic_diagram.png\" width=800>\n",
    "\n",
    "Current flow will:\n",
    "\n",
    "1. Take a user's message\n",
    "2. Create a system prompt with relevant Episodic enrichment\n",
    "3. Create a Semantic memory message with context from the database\n",
    "4. Reconstruct the entire working memory to update the system prompt and attach the semantic memory and new user messages to the end\n",
    "5. Generate a response with the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70d7380-62bc-40ef-97e1-355b647c45a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple storage for accumulated memories\n",
    "conversations = []\n",
    "what_worked = set()\n",
    "what_to_avoid = set()\n",
    "\n",
    "# Start Storage for Historical Message History\n",
    "messages = []\n",
    "\n",
    "while True:\n",
    "    # Get User's Message\n",
    "    user_input = input(\"\\nUser: \")\n",
    "    user_message = HumanMessage(content=user_input)\n",
    "    \n",
    "    # Generate new system prompt\n",
    "    system_prompt = episodic_system_prompt(user_input, vdb_client)\n",
    "    \n",
    "    # Reconstruct messages list with new system prompt first\n",
    "    messages = [\n",
    "        system_prompt,  # New system prompt always first\n",
    "        *[msg for msg in messages if not isinstance(msg, SystemMessage)]  # Old messages except system\n",
    "    ]\n",
    "    \n",
    "    if user_input.lower() == \"exit\":\n",
    "        add_episodic_memory(messages, vdb_client)\n",
    "        print(\"\\n == Conversation Stored in Episodic Memory ==\")\n",
    "        break\n",
    "    if user_input.lower() == \"exit_quiet\":\n",
    "        print(\"\\n == Conversation Exited ==\")\n",
    "        break\n",
    "    \n",
    "    # Get context and add it as a temporary message\n",
    "    context_message = semantic_rag(user_input, vdb_client)\n",
    "    \n",
    "    # Pass messages + context + user input to LLM\n",
    "    response = llm.invoke([*messages, context_message, user_message])\n",
    "    print(\"\\nAI Message: \", response.content)\n",
    "    \n",
    "    # Add only the user message and response to permanent history\n",
    "    messages.extend([user_message, response])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fe5955-24eb-4760-bbe0-b6860456e261",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(format_conversation(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b4f964-0f55-405d-bc40-9a51ed25409f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(context_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcf4829-a565-4715-abf0-1bd1be462b2c",
   "metadata": {},
   "source": [
    "---\n",
    "## Procedural Memory\n",
    "\n",
    "<img src=\"./media/procedural_memory.jpg\" width=400>\n",
    "\n",
    "*[10 Procedural Memory Examples](https://helpfulprofessor.com/procedural-memory-examples/)*\n",
    "\n",
    "Procedural memory is different from working, semantic, and episodic memory since it covers more how we actually remember to perform tasks or follow a familiar routine, i.e. riding a bike or typing on a keyboard. It's the \"how to do things\" type of memory, distinct from factual knowledge (semantic) or specific experiences (episodic). This memory system enables us to execute complex sequences of actions without conscious recall of each individual step.\n",
    "\n",
    "In terms of an LLM agent, procedural memory more abstractly consists of both its underlying language model weights and the framework code that defines information processing and response generation. The key difference from other memory types is that procedural memory encompasses the fundamental operations that make the system work - the core mechanisms that drive its behavior and capabilities.\n",
    "\n",
    "This takes two explicit forms: the learned patterns stored in the language model's weights from training, and the structured codebase that orchestrates memory interactions and shapes system behavior. Learning in procedural memory occurs through two main paths: adjustments to the language model's weights via fine-tuning or training, and updates to the system's core code. While fine-tuning enhances the model's language understanding and generation, code modifications can strengthen operations, enhance retrieval methods, or introduce new capabilities. These changes require careful implementation as they alter the system's fundamental operations.\n",
    "\n",
    "This procedural foundation also enables the integration of all memory systems. The language model's weights provide essential language processing abilities, while the framework code coordinates between working memory's current context, episodic memory's past experiences, and semantic memory's knowledge base. This architecture allows the system to transform understanding into effective action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277c457c-052a-4b1a-885a-7dd5a0191b2f",
   "metadata": {},
   "source": [
    "**Defining Permanent Instructions**\n",
    "\n",
    "Enabling an LLM to literally alter it's code and framework can be tricky to get right, we'll implement a smaller component of our overall system as an example, as well as more explicitly define our agent's structure. This will take the form of persistent instructions learned from prior interactions that will be attached as additional instructions, and updated as additional learnings from further conversations are created.\n",
    "\n",
    "We extend the original prompt with its episodic memory to now include procedural memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ded6cfc9-0339-40ab-97ab-6b4d90d8b44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def episodic_system_prompt(query, vdb_client):\n",
    "    # Get new memory\n",
    "    memory = episodic_recall(query, vdb_client)\n",
    "    \n",
    "    # Load Existing Procedural Memory Instructions\n",
    "    with open(\"./procedural_memory.txt\", \"r\") as content:\n",
    "        procedural_memory = content.read()\n",
    "    \n",
    "    # Get current conversation\n",
    "    current_conversation = memory.objects[0].properties['conversation']\n",
    "    \n",
    "    # Update memory stores, excluding current conversation from history\n",
    "    if current_conversation not in conversations:\n",
    "        conversations.append(current_conversation)\n",
    "    what_worked.update(memory.objects[0].properties['what_worked'].split('. '))\n",
    "    what_to_avoid.update(memory.objects[0].properties['what_to_avoid'].split('. '))\n",
    "    \n",
    "    # Get previous conversations excluding the current one\n",
    "    previous_convos = [conv for conv in conversations[-4:] if conv != current_conversation][-3:]\n",
    "    \n",
    "    # Create prompt with accumulated history\n",
    "    episodic_prompt = f\"\"\"You are a helpful AI Assistant. Answer the user's questions to the best of your ability.\n",
    "    You recall similar conversations with the user, here are the details:\n",
    "    \n",
    "    Current Conversation Match: {current_conversation}\n",
    "    Previous Conversations: {' | '.join(previous_convos)}\n",
    "    What has worked well: {' '.join(what_worked)}\n",
    "    What to avoid: {' '.join(what_to_avoid)}\n",
    "    \n",
    "    Use these memories as context for your response to the user.\n",
    "    \n",
    "    Additionally, here are 10 guidelines for interactions with the current user: {procedural_memory}\"\"\"\n",
    "    \n",
    "    return SystemMessage(content=episodic_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973cb345-3e27-4db1-b830-83c35dca332d",
   "metadata": {},
   "source": [
    "**Updating Procedural Memory**\n",
    "\n",
    "As a simple toy example, we will take in our existing list, add the running list of what we've learned across conversation from episodic memory, and update our list of procedural memories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b4ef9802-456a-4254-8210-161ad284726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def procedural_memory_update(what_worked, what_to_avoid):\n",
    "\n",
    "    # Load Existing Procedural Memory Instructions\n",
    "    with open(\"./procedural_memory.txt\", \"r\") as content:\n",
    "        current_takeaways = content.read()\n",
    "\n",
    "    # Load Existing and Gathered Feedback into Prompt\n",
    "    procedural_prompt = f\"\"\"You are maintaining a continuously updated list of the most important procedural behavior instructions for an AI assistant. Your task is to refine and improve a list of key takeaways based on new conversation feedback while maintaining the most valuable existing insights.\n",
    "\n",
    "    CURRENT TAKEAWAYS:\n",
    "    {current_takeaways}\n",
    "\n",
    "    NEW FEEDBACK:\n",
    "    What Worked Well:\n",
    "    {what_worked}\n",
    "\n",
    "    What To Avoid:\n",
    "    {what_to_avoid}\n",
    "\n",
    "    Please generate an updated list of up to 10 key takeaways that combines:\n",
    "    1. The most valuable insights from the current takeaways\n",
    "    2. New learnings from the recent feedback\n",
    "    3. Any synthesized insights combining multiple learnings\n",
    "\n",
    "    Requirements for each takeaway:\n",
    "    - Must be specific and actionable\n",
    "    - Should address a distinct aspect of behavior\n",
    "    - Include a clear rationale\n",
    "    - Written in imperative form (e.g., \"Maintain conversation context by...\")\n",
    "\n",
    "    Format each takeaway as:\n",
    "    [#]. [Instruction] - [Brief rationale]\n",
    "\n",
    "    The final list should:\n",
    "    - Be ordered by importance/impact\n",
    "    - Cover a diverse range of interaction aspects\n",
    "    - Focus on concrete behaviors rather than abstract principles\n",
    "    - Preserve particularly valuable existing takeaways\n",
    "    - Incorporate new insights when they provide meaningful improvements\n",
    "\n",
    "    Return up to but no more than 10 takeaways, replacing or combining existing ones as needed to maintain the most effective set of guidelines.\n",
    "    Return only the list, no preamble or explanation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate New Procedural Memory\n",
    "    procedural_memory = llm.invoke(procedural_prompt)\n",
    "\n",
    "    # Write to File\n",
    "    with open(\"./procedural_memory.txt\", \"w\") as content:\n",
    "        content.write(procedural_memory.content)\n",
    "\n",
    "    return\n",
    "\n",
    "# prompt = procedural_memory_update(what_worked, what_to_avoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e9504e-17f8-4199-b13c-27f08d8ac30a",
   "metadata": {},
   "source": [
    "**Full Working Memory Demonstration**\n",
    "\n",
    "<img src=\"./media/procedural_diagram.png\" width=800>\n",
    "\n",
    "Current flow will:\n",
    "\n",
    "1. Take a user's message\n",
    "2. Create a system prompt with relevant Episodic enrichment\n",
    "3. Insert procedural memory into prompt\n",
    "4. Create a Semantic memory message with context from the database\n",
    "5. Reconstruct the entire working memory to update the system prompt and attach the semantic memory and new user messages to the end\n",
    "6. Generate a response with the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0eadcd-ee4f-480f-9bae-28f4bb5dc774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple storage for accumulated memories\n",
    "conversations = []\n",
    "what_worked = set()\n",
    "what_to_avoid = set()\n",
    "\n",
    "# Start Storage for Historical Message History\n",
    "messages = []\n",
    "\n",
    "while True:\n",
    "    # Get User's Message\n",
    "    user_input = input(\"\\nUser: \")\n",
    "    user_message = HumanMessage(content=user_input)\n",
    "    \n",
    "    # Generate new system prompt\n",
    "    system_prompt = episodic_system_prompt(user_input, vdb_client)\n",
    "    \n",
    "    # Reconstruct messages list with new system prompt first\n",
    "    messages = [\n",
    "        system_prompt,  # New system prompt always first\n",
    "        *[msg for msg in messages if not isinstance(msg, SystemMessage)]  # Old messages except system\n",
    "    ]\n",
    "    \n",
    "    if user_input.lower() == \"exit\":\n",
    "        add_episodic_memory(messages, vdb_client)\n",
    "        print(\"\\n == Conversation Stored in Episodic Memory ==\")\n",
    "        procedural_memory_update(what_worked, what_to_avoid)\n",
    "        print(\"\\n== Procedural Memory Updated ==\")\n",
    "        break\n",
    "    if user_input.lower() == \"exit_quiet\":\n",
    "        print(\"\\n == Conversation Exited ==\")\n",
    "        break\n",
    "    \n",
    "    # Get context and add it as a temporary message\n",
    "    context_message = semantic_rag(user_input, vdb_client)\n",
    "    \n",
    "    # Pass messages + context + user input to LLM\n",
    "    response = llm.invoke([*messages, context_message, user_message])\n",
    "    print(\"\\nAI Message: \", response.content)\n",
    "    \n",
    "    # Add only the user message and response to permanent history\n",
    "    messages.extend([user_message, response])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5aba41-0f13-4e7d-9b27-d075a75e85fc",
   "metadata": {},
   "source": [
    "**Looking At The Conversation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9f7a73-4cf5-414b-8f59-8a7a6145dac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(format_conversation(messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0719b8e1-3e59-47de-90f6-5a6d60ca128b",
   "metadata": {},
   "source": [
    "**Looking At Current System Prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e83e46f-a349-4675-9b7f-b9388f192eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(system_prompt.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c034480-abd7-4720-bb70-97d0d6dc6400",
   "metadata": {},
   "source": [
    "**Looking At the Context Message**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35038cec-8090-4411-91d2-a43ba3ca5c70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(context_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2423efc-53c3-4653-9a35-b78f97ab2b37",
   "metadata": {},
   "source": [
    "---\n",
    "## Discussion\n",
    "\n",
    "<img src=\"./media/memory_types.jpg\" width=500>\n",
    "\n",
    "Memory systems enable us to move beyond using LLMs as simple input/output models into agents that can operate with forms of persistent understanding and learning. Each memory type serves a distinct cognitive purpose:\n",
    "\n",
    "**Working Memory**\n",
    "The immediate cognitive workspace - keeping track of and contextualizing what's happening right now. For LLMs specifically, this combats the stateless nature of model calls by maintaining active context.\n",
    "\n",
    "**Episodic Memory** \n",
    "Historical experiences and their associated learnings. Not just storing past events, but also the ability to reflect on and learn from them through applying your memory of similar episodes to new experiences. Allows LLMs to extract meaningful patterns and insights from experiences and use them in the future.\n",
    "\n",
    "**Semantic Memory**\n",
    "Pure knowledge representation, separate from specific experiences. While LLMs have knowledge baked into their weights, semantic memory provides explicit, queryable facts that can ground responses. This enables dynamic knowledge integration rather than relying solely on training data.\n",
    "\n",
    "**Procedural Memory**\n",
    "Both implicit in model weights and explicit in code, this shapes how the other memory systems are used and how the agent actually executes behaviors. Unlike the other memory types, changes here fundamentally alter how the agent functions.\n",
    "\n",
    "Together, working memory actively manipulates current context, retrieving relevant experiences from episodic memory, grounding in semantic knowledge, all guided by procedural patterns. Each type builds on the others to enable increasingly sophisticated cognitive capabilities with LLM system design."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b048516e-665d-4eab-b888-c52a278ea96e",
   "metadata": {},
   "source": [
    "---\n",
    "## Additional Examples of Memory Implementation from Research\n",
    "\n",
    "For a comprehensive survey of advanced memory techniques and applications, check out [A Survey on the Memory Mechanism of Large\n",
    "Language Model based Agents](https://arxiv.org/pdf/2404.13501)! Here are a few notable mentions:\n",
    "\n",
    "#### [MemGPT: Towards LLMs as Operating Systems](https://arxiv.org/pdf/2310.08560)\n",
    "\n",
    "<img src=\"./media/mem_gpt.png\" width=600>\n",
    "\n",
    "MemGPT is a system that enables large language models (LLMs) to handle context beyond their fixed context window limits by implementing a hierarchical memory system inspired by traditional operating systems. Just as operating systems use virtual memory to page data between physical memory and disk, MemGPT manages different storage tiers to effectively extend an LLM's limited context window. The system has three main memory components: a read-only system instructions section, a read/write working context for storing key information, and a FIFO queue for message history - all within the LLM's main context window (analogous to RAM). When this main context approaches capacity, MemGPT can move less immediately relevant information to external \"archival storage\" and \"recall storage\" (analogous to disk storage). The system uses function calls to intelligently manage what information stays in the main context versus what gets moved to external storage, and can retrieve relevant information back into the main context when needed through search and pagination mechanisms.\n",
    "\n",
    "#### [VOYAGER: An Open-Ended Embodied Agent with Large Language Models](https://arxiv.org/pdf/2305.16291)\n",
    "\n",
    "<img src=\"./media/voyager.png\" width=600>\n",
    "\n",
    "Voyager is an autonomous AI agent that explores and learns to play Minecraft using GPT-4 as its core reasoning engine. Its memory and learning system centers around three key components: an automatic curriculum that proposes appropriately challenging tasks based on the agent's current capabilities, a skill library that stores successful code programs as reusable skills, and an iterative prompting mechanism that refines actions through environmental feedback. The skill library acts as Voyager's long-term memory, where each mastered skill is stored as executable code indexed by embeddings of its description, allowing relevant skills to be retrieved and composed into more complex behaviors when facing new challenges. Through this system, Voyager accumulates knowledge by storing successful code patterns rather than relying on traditional parameter updates or gradient-based learning, enabling it to continually build upon its capabilities while avoiding catastrophic forgetting. This approach allows Voyager to organically explore and master increasingly sophisticated tasks, from basic resource gathering to complex tool crafting, while maintaining the ability to reuse and adapt its learned skills in new situations.\n",
    "\n",
    "#### [Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term Memory](https://arxiv.org/pdf/2311.08719)\n",
    "\n",
    "<img src=\"./media/tim.png\" width=600>\n",
    "\n",
    "TiM (Think-in-Memory) is a memory mechanism for Large Language Models (LLMs) that enables more consistent long-term memory by storing and recalling thoughts rather than raw conversation history. Instead of repeatedly reasoning over past conversations, TiM operates in two key stages: first, it recalls relevant thoughts from memory before generating a response, and second, it performs \"post-thinking\" after generating a response to update its memory with new thoughts. These thoughts are stored using Locality-Sensitive Hashing (LSH) for efficient retrieval and organization. The system supports three main operations: inserting new thoughts, forgetting unnecessary ones, and merging similar thoughts. By storing processed thoughts rather than raw conversations, TiM avoids the inconsistency problems that can arise from repeatedly reasoning over the same history in different ways, while also making retrieval more efficient since it only needs to search within relevant thought clusters.\n",
    "\n",
    "#### [Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization](https://arxiv.org/pdf/2308.02151)\n",
    "\n",
    "<img src=\"./media/retroformer.png\" width=600>\n",
    "\n",
    "Retroformer is a framework for improving large language model (LLM) agents through a plug-in retrospective model that automatically refines agent prompts based on environmental feedback. Its memory system works through three components: 1) short-term memory from the trajectory history of the current episode, 2) long-term memory from self-reflection responses that summarize prior failed attempts and are appended to the actor prompt, and 3) a replay buffer that stores triplets of reflection prompts, responses, and episode returns across different tasks and environments. The retrospective model uses policy gradient optimization to learn from these memories - it analyzes failed attempts, generates reflective feedback, and updates its parameters to produce better prompting that helps the agent avoid past mistakes. Rather than trying to modify the core LLM agent (which remains frozen), Retroformer focuses on optimizing this retrospective component to provide better guidance through refined prompts, allowing the agent to improve over time while maintaining its original capabilities.\n",
    "\n",
    "### [MemoryBank: Enhancing Large Language Models with Long-Term Memory](https://arxiv.org/pdf/2305.10250)\n",
    "\n",
    "<img src=\"./media/memorybank.png\" width=600>\n",
    "\n",
    "MemoryBank is a long-term memory system designed for Large Language Models that consists of three core components: a memory storage system, a memory retrieval mechanism, and a memory updating system inspired by human cognition. The memory storage maintains detailed conversation logs, event summaries, and evolving user personality profiles in a hierarchical structure. When new interactions occur, a dual-tower dense retrieval model (similar to Dense Passage Retrieval) encodes both the current context and stored memories into vector representations, then uses FAISS indexing to efficiently retrieve relevant past information. The system uniquely incorporates an Ebbinghaus Forgetting Curve-based updating mechanism that allows memories to naturally decay over time unless reinforced through repeated recall, mimicking human memory patterns. The memory strength is modeled as a discrete value that increases when information is recalled, with memories becoming more resistant to forgetting through repeated access. This comprehensive approach enables LLMs to maintain context over extended periods, understand user personalities, and provide more personalized interactions while simulating natural memory retention and decay patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73955533-3bdd-4b6f-91db-0af790967beb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
